import json
from datetime import datetime
import re
from collections import Counter

ESSENTIAL_FIELDS = [
    "room_name",
    "address",
    "price_per_month",
    "area",
    "description",
    "extensions",
    "full_furnishing",
    "start_date",
    "expire",
    "latitude",
    "longitude",
    "type",
    "url"
]

# Danh s√°ch c√°c c·ª•m t·ª´ mang t√≠nh PR c·∫ßn lo·∫°i b·ªè
PR_KEYWORDS = [
    "si√™u ∆∞u ƒë√£i", "gi√° ch·ªâ", "ch·ªâ c√≤n", "bao t·∫•t c·∫£", "bao full", "ƒë·∫£m b·∫£o", "mi·ªÖn ph√≠", "∆∞u ƒë√£i", 
    "c·ª±c k·ª≥", "v√¥ c√πng", "trung t√¢m", "si√™u r·∫ª", "si√™u ƒë·∫πp", "tho·∫£i m√°i", "t·ª± do", "ti·ªán nghi", 
    "sang tr·ªçng", "cao c·∫•p", "full n·ªôi th·∫•t", "r·ªông r√£i", "an ninh", "s·∫°ch s·∫Ω", "gi·ªù gi·∫•c t·ª± do", 
    "v·ªã tr√≠ ƒë·∫Øc ƒë·ªãa", "d·ªãch v·ª• t·ªët", "nhi·ªát t√¨nh", "h·ªó tr·ª£", "qu·∫£n l√Ω 24/7"
]
def preprocess_date(date_str):
    """
    Chuy·ªÉn chu·ªói date_str d·∫°ng "HH:MM dd/mm/yyyy" th√†nh "yyyy-mm-dd" (ch·ªâ l·∫•y ng√†y).
    N·∫øu kh√¥ng parse ƒë∆∞·ª£c tr·∫£ v·ªÅ None.
    """
    try:
        parts = date_str.split(",")[-1].strip()
        dt = datetime.strptime(parts, "%H:%M %d/%m/%Y")
        return dt.date().isoformat()  # ch·ªâ l·∫•y ph·∫ßn ng√†y
    except:
        return None


def clean_text(text):
    """
    L√†m s·∫°ch m√¥ t·∫£:
    - Xo√° k√Ω hi·ªáu, emoji, s·ªë ƒëi·ªán tho·∫°i, email
    - B·ªè c√°c c·ª•m t·ª´ mang t√≠nh PR
    """
    text = re.sub(r"[^\w\s,.‚Äì-]", "", text)
    text = re.sub(r"[-=]{2,}", " ", text)
    text = re.sub(r"[*]{2,}", " ", text)
    text = re.sub(r"\b\d{8,11}\b", "", text)  # SƒêT
    text = re.sub(r"\S+@\S+", "", text)       # Email
    text = re.sub(r"Li√™n h·ªá.*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"(ƒê·ªäA CH·ªà|H·ªÜ TH·ªêNG|BE HOME).*", "", text, flags=re.IGNORECASE)

    # B·ªè c·ª•m PR
    for phrase in PR_KEYWORDS:
        pattern = re.compile(re.escape(phrase), re.IGNORECASE)
        text = pattern.sub("", text)

    text = re.sub(r"\s{2,}", " ", text).strip()
    return text

def preprocess_room(room):
    new_room = {}

    # X·ª≠ l√Ω start_date tr∆∞·ªõc
    start_date_str = room.get("start_date", None)
    start_date = preprocess_date(start_date_str) if start_date_str else None

    # X·ª≠ l√Ω expire tr∆∞·ªõc
    expire_str = room.get("expire", None)
    expire = preprocess_date(expire_str) if expire_str else None

    for key in ESSENTIAL_FIELDS:
        if key in room:
            if key == "start_date":
                new_room[key] = start_date
            elif key == "expire":
                # N·∫øu expire nh·ªè h∆°n start_date ho·∫∑c None th√¨ b·ªè expire
                if expire and start_date and expire >= start_date:
                    new_room[key] = expire
                else:
                    # B·ªè tr∆∞·ªùng expire n·∫øu expire < start_date ho·∫∑c expire None
                    pass
            elif key == "area":
                area_str = str(room[key])
                match = re.search(r"(\d+)", area_str)
                new_room[key] = int(match.group(1)) if match else 0
            else:
                new_room[key] = room[key]

    # combined_text ch·ªâ l·∫•y t·ª´ description
    if "description" in new_room:
        raw_text = "\n".join(new_room["description"])
        combined_text = clean_text(raw_text)
    else:
        combined_text = ""

    new_room["combined_text"] = combined_text
    new_room.pop("description", None)

    return new_room

def get_top_keywords(rooms, top_n=20):
    all_words = []
    for room in rooms:
        text = room.get("combined_text", "")
        words = re.findall(r"\w+", text.lower())
        filtered = [w for w in words if len(w) > 2]
        all_words.extend(filtered)

    return Counter(all_words).most_common(top_n)

def get_top_keywords_by_type(rooms, top_n=10):
    type_word_map = {}

    for room in rooms:
        room_type = room.get("type", "unknown").lower()
        text = room.get("combined_text", "")
        words = re.findall(r"\w+", text.lower())
        filtered = [w for w in words if len(w) > 2]

        if room_type not in type_word_map:
            type_word_map[room_type] = []

        type_word_map[room_type].extend(filtered)

    result = {}
    for room_type, words in type_word_map.items():
        result[room_type] = Counter(words).most_common(top_n)

    return result

def preprocess_all_rooms(input_file, output_file,
                         top_keywords_file="top_keywords.json",
                         type_keywords_file="top_keywords_by_type.json"):
    with open(input_file, "r", encoding="utf-8") as f:
        rooms = json.load(f)

    processed = [preprocess_room(room) for room in rooms]

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(processed, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Ti·ªÅn x·ª≠ l√Ω xong. {len(processed)} ph√≤ng ƒë∆∞·ª£c l∆∞u v√†o {output_file}")

    top_keywords = get_top_keywords(processed)
    with open(top_keywords_file, "w", encoding="utf-8") as f:
        json.dump(top_keywords, f, ensure_ascii=False, indent=2)

    print(f"\nüîç Top {len(top_keywords)} t·ª´ kh√≥a ph·ªï bi·∫øn:")
    for i, (word, freq) in enumerate(top_keywords, 1):
        print(f"{i:2}. {word} ({freq})")

    type_keywords = get_top_keywords_by_type(processed)
    with open(type_keywords_file, "w", encoding="utf-8") as f:
        json.dump(type_keywords, f, ensure_ascii=False, indent=2)

    print("\nüìä T·ª´ kh√≥a ph·ªï bi·∫øn theo t·ª´ng lo·∫°i ph√≤ng:")
    for room_type, keywords in type_keywords.items():
        keyword_str = ", ".join([f"{word} ({count})" for word, count in keywords])
        print(f"- {room_type}: {keyword_str}")

if __name__ == "__main__":
    preprocess_all_rooms(
        input_file="cleaned_roomInformation.json",
        output_file="preprocessed_roomInformation.json"
    )
